import os
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Import the functions from the previous task (import only in the __main__ block to avoid circular dependencies and import issues)
# from .ingestion import load_documents_from_directory


def split_documents_into_chunks(documents: List[Dict[str, str]],
                                 chunk_size: int = 1000,
                                 chunk_overlap: int = 200) -> List[Dict[str, str]]:
    """
    Split the loaded document list into smaller chunks.

    Args:
        documents (List[Dict[str, str]]): A list of documents obtained from the loader, each dictionary containing 'text' and 'source'.
        chunk_size (int): Maximum number of characters per text block.
        chunk_overlap (int): The number of characters of overlap between adjacent blocks of text, to help preserve context.

    Returns:
        List[Dict[str, str]]: A list of text chunks, each dictionary containing 'text' and 'metadata' (including 'source').
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,  # Use the len function to calculate the length
        add_start_index=True, # Add the starting position of the chunk in the original text for easy tracking
    )

    all_chunks = []
    for doc in documents:
        # Before chunking, make sure the document content is not empty or contains only whitespace characters
        if not doc["text"] or doc["text"].strip() == "":
            print(f"Skipping empty or whitespace-only document from {doc['source']}.")
            continue

        # Split document text into chunks
        # Langchain's create_documents expects a list of strings as first argument
        # and a list of metadata dictionaries as metadatas argument
        chunks = text_splitter.create_documents([doc["text"]], metadatas=[{"source": doc["source"]}])
        all_chunks.extend(chunks)
        print(f"Split document from {doc['source']} into {len(chunks)} chunks.")

    # Convert the Langchain Document object to our custom dictionary format
    # At the same time, filter out empty or blank chunks that may be generated by the splitter
    formatted_chunks = []
    for chunk in all_chunks:
        chunk_content = chunk.page_content.strip() # Remove leading/trailing whitespace
        if chunk_content: # Ensure content is not empty
            formatted_chunks.append({
                "text": chunk_content,
                "metadata": chunk.metadata
            })
        else:
            print(f"Skipping empty or whitespace-only chunk from {chunk.metadata.get('source', 'unknown source')}.")

    return formatted_chunks

if __name__ == "__main__":
    # This is a simple test area to verify the chunking functionality

    # Import functions from the previous task here so they can be found when the file is run directly
    # Ensure that src and its subdirectories all contain __init__.py files
    from src.data_processing.ingestion import load_documents_from_directory

    # Construct the raw data path (same as in ingestion.py)
    current_file_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_file_dir, '..', '..'))
    raw_data_path = os.path.join(project_root, 'data', 'raw')

    print(f"Loading documents from: {raw_data_path}")
    loaded_docs = load_documents_from_directory(raw_data_path)

    if loaded_docs:
        print("\nSplitting loaded documents into chunks...")
        # We can try different chunk_size and chunk_overlap
        chunks = split_documents_into_chunks(loaded_docs, chunk_size=500, chunk_overlap=100)

        print(f"\nSuccessfully created {len(chunks)} chunks in total.")

        # Print a preview of a few text blocks
        print("\n--- Chunk Previews ---")
        for i, chunk in enumerate(chunks[:5]): # Print only the first 5 chunks as preview
            print(f"Chunk {i+1} (Source: {chunk['metadata']['source']}):")
            # Safely print the first 150 characters as a preview, handling potential unencodable characters
            preview_text = chunk['text'][:150]
            try:
                print(f"Content: {preview_text.encode('utf-8', 'ignore').decode('utf-8')}...")
            except Exception as e:
                print(f"Content (encoding issue): {e} - Showing raw first 50 chars: {chunk['text'][:50]}...")
            print("-" * 30)

        if len(chunks) > 5:
            print(f"... and {len(chunks) - 5} more chunks.")

    else:
        print("No documents loaded to split. Please check Task 2.1.")